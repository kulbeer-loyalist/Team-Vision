{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a01ab58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-08 11:08:28.872243: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-08 11:08:29.710152: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_4718/901780240.py:4: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-08 11:08:31.156587: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2023-08-08 11:08:31.156620: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:168] retrieving CUDA diagnostic information for host: langchain\n",
      "2023-08-08 11:08:31.156627: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:175] hostname: langchain\n",
      "2023-08-08 11:08:31.156808: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:199] libcuda reported version is: 520.56.6\n",
      "2023-08-08 11:08:31.156826: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:203] kernel reported version is: 520.56.6\n",
      "2023-08-08 11:08:31.156830: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:309] kernel version seems to match DSO: 520.56.6\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "gpu_available = tf.test.is_gpu_available() \n",
    "print(gpu_available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09df95cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-08 08:32:38.296241: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-08 08:32:39.098777: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_holistic = mp.solutions.holistic\n",
    "\n",
    "\n",
    "\n",
    "def create_landmark_frame_df(results,frame,xyz):\n",
    "    \"\"\"\n",
    "    xyz takes the results from mediapipe and creates a dataframe of the landmark\n",
    "    \n",
    "    inputs:\n",
    "        results: mediapipe results object\n",
    "        frame: frame number\n",
    "        xyz: dataframe wof the xyz example data\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #we want the values and rows for every type landmark index so we need skeleton\n",
    "    xyz_skel = xyz[['type','landmark_index']].drop_duplicates() \\\n",
    "    .reset_index(drop=True).copy()\n",
    "    \n",
    "    pose = pd.DataFrame()\n",
    "    face = pd.DataFrame()\n",
    "    left_hand = pd.DataFrame()\n",
    "    right_hand = pd.DataFrame()\n",
    "    \n",
    "    if results.face_landmarks is not None:\n",
    "        for i, point in enumerate(results.face_landmarks.landmark):\n",
    "            face.loc[i, ['x','y','z']] = [point.x, point.y, point.z]\n",
    "\n",
    "    if results.pose_landmarks is not None:\n",
    "        for i, point in enumerate(results.pose_landmarks.landmark):\n",
    "            pose.loc[i, ['x','y','z']] = [point.x, point.y, point.z]\n",
    "\n",
    "\n",
    "    if results.left_hand_landmarks is not None:\n",
    "        for i, point in enumerate(results.left_hand_landmarks.landmark):\n",
    "            left_hand.loc[i, ['x','y','z']] = [point.x, point.y, point.z]\n",
    "\n",
    "\n",
    "    if results.right_hand_landmarks is not None:\n",
    "        for i, point in enumerate(results.right_hand_landmarks.landmark):\n",
    "            right_hand.loc[i, ['x','y','z']] = [point.x, point.y, point.z]\n",
    "\n",
    "    face = face.reset_index() \\\n",
    "    .rename(columns={'index': 'landmark_index'}) \\\n",
    "    .assign(type='face')\n",
    "\n",
    "    pose = pose.reset_index() \\\n",
    "    .rename(columns={'index': 'landmark_index'}) \\\n",
    "    .assign(type='pose')\n",
    "\n",
    "    left_hand = left_hand.reset_index() \\\n",
    "    .rename(columns={'index': 'landmark_index'}) \\\n",
    "    .assign(type='left_hand')\n",
    "\n",
    "    right_hand = right_hand.reset_index() \\\n",
    "    .rename(columns={'index': 'landmark_index'}) \\\n",
    "    .assign(type='right_hand')\n",
    "\n",
    "    landmark = pd.concat([face,pose,left_hand,right_hand]).reset_index(drop=True)\n",
    "    #merge with lanndmark\n",
    "    landmark = xyz_skel.merge(landmark, on=['type','landmark_index'], how='left')\n",
    "    #assign frames to make it unique\n",
    "    landmark = landmark.assign(frame = frame)\n",
    "    return landmark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e252568c",
   "metadata": {},
   "source": [
    "# working code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c356990",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot convert float NaN to integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 76\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     75\u001b[0m     xyz \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./2044/635217.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 76\u001b[0m     landmark \u001b[38;5;241m=\u001b[39m \u001b[43mdo_capture_web\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxyz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     landmark \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(landmark)\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     78\u001b[0m     landmark\u001b[38;5;241m.\u001b[39mto_parquet(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput2.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 60\u001b[0m, in \u001b[0;36mdo_capture_web\u001b[0;34m(xyz)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Iterate through the tracked landmarks\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, landmark_row \u001b[38;5;129;01min\u001b[39;00m landmark_df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m---> 60\u001b[0m     landmark_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlandmark_row\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     landmark_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(landmark_row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     62\u001b[0m     landmark_z \u001b[38;5;241m=\u001b[39m landmark_row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mz\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: cannot convert float NaN to integer"
     ]
    }
   ],
   "source": [
    "def do_capture_web(xyz):\n",
    "    all_landmarks = []\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5,min_tracking_confidence=0.5) as holistic:\n",
    "        frame = 0  \n",
    "        while cap.isOpened():\n",
    "                #take frame and increment it\n",
    "            frame+=1\n",
    "            success, image = cap.read()\n",
    "            if not success:\n",
    "                print(\"Ignoring empty camera frame.\")\n",
    "                    # If loading a video, use 'break' instead of 'continue'.\n",
    "                continue\n",
    "\n",
    "                # To improve performance, optionally mark the image as not writeable to\n",
    "                # pass by reference.\n",
    "            image.flags.writeable = False\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            results = holistic.process(image)\n",
    "            #create landmark dataframe\n",
    "            landmark_df = create_landmark_frame_df(results, frame, xyz)\n",
    "            all_landmarks.append(landmark_df)\n",
    "\n",
    "            # Draw landmark annotation on the image.\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image,\n",
    "                results.face_landmarks,\n",
    "                mp_holistic.FACEMESH_CONTOURS,\n",
    "                landmark_drawing_spec=None,\n",
    "                connection_drawing_spec=mp_drawing_styles\n",
    "                .get_default_face_mesh_contours_style())\n",
    "\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image,\n",
    "                results.pose_landmarks,\n",
    "                mp_holistic.POSE_CONNECTIONS,\n",
    "                landmark_drawing_spec=mp_drawing_styles\n",
    "                .get_default_pose_landmarks_style())\n",
    "\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image,\n",
    "                results.left_hand_landmarks,\n",
    "                mp_holistic.HAND_CONNECTIONS,\n",
    "                landmark_drawing_spec=mp_drawing_styles\n",
    "                .get_default_hand_landmarks_style())\n",
    "\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image,\n",
    "                results.right_hand_landmarks,\n",
    "                mp_holistic.HAND_CONNECTIONS,\n",
    "                landmark_drawing_spec=mp_drawing_styles\n",
    "                .get_default_hand_landmarks_style())\n",
    "            \n",
    "            # Iterate through the tracked landmarks\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "                # Flip the image horizontally for a selfie-view display.\n",
    "            cv2.imshow('MediaPipe Holistic', cv2.flip(image, 1))\n",
    "            if cv2.waitKey(5) & 0xFF == 27:\n",
    "                break\n",
    "                \n",
    "\n",
    "    return all_landmarks    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    xyz = pd.read_parquet('./2044/635217.parquet')\n",
    "    landmark = do_capture_web(xyz)\n",
    "    landmark = pd.concat(landmark).reset_index(drop=True)\n",
    "    landmark.to_parquet('output2.parquet')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ba7a99",
   "metadata": {},
   "source": [
    "# dummy code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "401513dd",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Cannot run the event loop while another loop is running",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 167\u001b[0m\n\u001b[1;32m    165\u001b[0m     loop \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mnew_event_loop()\n\u001b[1;32m    166\u001b[0m     asyncio\u001b[38;5;241m.\u001b[39mset_event_loop(loop)\n\u001b[0;32m--> 167\u001b[0m     \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdo_capture_web_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# If Esc is pressed, stop the capture task and exit gracefully\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcess interrupted by Esc key.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/asyncio/base_events.py:625\u001b[0m, in \u001b[0;36mBaseEventLoop.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run until the Future is done.\u001b[39;00m\n\u001b[1;32m    615\u001b[0m \n\u001b[1;32m    616\u001b[0m \u001b[38;5;124;03mIf the argument is a coroutine, it is wrapped in a Task.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;124;03mReturn the Future's result, or raise its exception.\u001b[39;00m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[0;32m--> 625\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_running\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    627\u001b[0m new_task \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m futures\u001b[38;5;241m.\u001b[39misfuture(future)\n\u001b[1;32m    628\u001b[0m future \u001b[38;5;241m=\u001b[39m tasks\u001b[38;5;241m.\u001b[39mensure_future(future, loop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/asyncio/base_events.py:586\u001b[0m, in \u001b[0;36mBaseEventLoop._check_running\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThis event loop is already running\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    585\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 586\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    587\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot run the event loop while another loop is running\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Cannot run the event loop while another loop is running"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_holistic = mp.solutions.holistic\n",
    "\n",
    "\n",
    "\n",
    "def create_landmark_frame_df(results,frame,xyz):\n",
    "    \"\"\"\n",
    "    xyz takes the results from mediapipe and creates a dataframe of the landmark\n",
    "    \n",
    "    inputs:\n",
    "        results: mediapipe results object\n",
    "        frame: frame number\n",
    "        xyz: dataframe wof the xyz example data\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #we want the values and rows for every type landmark index so we need skeleton\n",
    "    xyz_skel = xyz[['type','landmark_index']].drop_duplicates() \\\n",
    "    .reset_index(drop=True).copy()\n",
    "    \n",
    "    pose = pd.DataFrame()\n",
    "    face = pd.DataFrame()\n",
    "    left_hand = pd.DataFrame()\n",
    "    right_hand = pd.DataFrame()\n",
    "    \n",
    "    if results.face_landmarks is not None:\n",
    "        for i, point in enumerate(results.face_landmarks.landmark):\n",
    "            face.loc[i, ['x','y','z']] = [point.x, point.y, point.z]\n",
    "\n",
    "    if results.pose_landmarks is not None:\n",
    "        for i, point in enumerate(results.pose_landmarks.landmark):\n",
    "            pose.loc[i, ['x','y','z']] = [point.x, point.y, point.z]\n",
    "\n",
    "\n",
    "    if results.left_hand_landmarks is not None:\n",
    "        for i, point in enumerate(results.left_hand_landmarks.landmark):\n",
    "            left_hand.loc[i, ['x','y','z']] = [point.x, point.y, point.z]\n",
    "\n",
    "\n",
    "    if results.right_hand_landmarks is not None:\n",
    "        for i, point in enumerate(results.right_hand_landmarks.landmark):\n",
    "            right_hand.loc[i, ['x','y','z']] = [point.x, point.y, point.z]\n",
    "\n",
    "    face = face.reset_index() \\\n",
    "    .rename(columns={'index': 'landmark_index'}) \\\n",
    "    .assign(type='face')\n",
    "\n",
    "    pose = pose.reset_index() \\\n",
    "    .rename(columns={'index': 'landmark_index'}) \\\n",
    "    .assign(type='pose')\n",
    "\n",
    "    left_hand = left_hand.reset_index() \\\n",
    "    .rename(columns={'index': 'landmark_index'}) \\\n",
    "    .assign(type='left_hand')\n",
    "\n",
    "    right_hand = right_hand.reset_index() \\\n",
    "    .rename(columns={'index': 'landmark_index'}) \\\n",
    "    .assign(type='right_hand')\n",
    "\n",
    "    landmark = pd.concat([face,pose,left_hand,right_hand]).reset_index(drop=True)\n",
    "    #merge with lanndmark\n",
    "    landmark = xyz_skel.merge(landmark, on=['type','landmark_index'], how='left')\n",
    "    #assign frames to make it unique\n",
    "    landmark = landmark.assign(frame = frame)\n",
    "    return landmark\n",
    "\n",
    "async def do_capture_web_async():\n",
    "    all_landmarks = []\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    xyz = pd.read_parquet('./2044/635217.parquet')\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5,min_tracking_confidence=0.5) as holistic:\n",
    "        frame = 0  \n",
    "        while cap.isOpened():\n",
    "                #take frame and increment it\n",
    "            frame+=1\n",
    "            success, image = cap.read()\n",
    "            if not success:\n",
    "                print(\"Ignoring empty camera frame.\")\n",
    "                    # If loading a video, use 'break' instead of 'continue'.\n",
    "                continue\n",
    "\n",
    "                # To improve performance, optionally mark the image as not writeable to\n",
    "                # pass by reference.\n",
    "            image.flags.writeable = False\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            results = holistic.process(image)\n",
    "            #create landmark dataframe\n",
    "            landmark = create_landmark_frame_df(results, frame, xyz)\n",
    "            all_landmarks.append(landmark)\n",
    "            \n",
    "\n",
    "            # Draw landmark annotation on the image.\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image,\n",
    "                results.face_landmarks,\n",
    "                mp_holistic.FACEMESH_CONTOURS,\n",
    "                landmark_drawing_spec=None,\n",
    "                connection_drawing_spec=mp_drawing_styles\n",
    "                .get_default_face_mesh_contours_style())\n",
    "\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image,\n",
    "                results.pose_landmarks,\n",
    "                mp_holistic.POSE_CONNECTIONS,\n",
    "                landmark_drawing_spec=mp_drawing_styles\n",
    "                .get_default_pose_landmarks_style())\n",
    "\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image,\n",
    "                results.left_hand_landmarks,\n",
    "                mp_holistic.HAND_CONNECTIONS,\n",
    "                landmark_drawing_spec=mp_drawing_styles\n",
    "                .get_default_hand_landmarks_style())\n",
    "\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image,\n",
    "                results.right_hand_landmarks,\n",
    "                mp_holistic.HAND_CONNECTIONS,\n",
    "                landmark_drawing_spec=mp_drawing_styles\n",
    "                .get_default_hand_landmarks_style())\n",
    "\n",
    "                # Flip the image horizontally for a selfie-view display.\n",
    "            cv2.imshow('MediaPipe Holistic', cv2.flip(image, 1))\n",
    "            if cv2.waitKey(5) & 0xFF == 27:\n",
    "                break\n",
    "        \n",
    "\n",
    "    cap.release()\n",
    "    xyz_np = np.array(all_landmarks).astype(np.float32)\n",
    "\n",
    "        # Perform prediction\n",
    "    await predict_and_print(xyz_np)\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "# Function to perform prediction and print the result\n",
    "def predict_and_print(landmark):\n",
    "    # Load the model and get the prediction_fn\n",
    "    interpreter = tf.lite.Interpreter(model_path=\"./model.tflite\")\n",
    "    prediction_fn = interpreter.get_signature_runner(\"serving_default\")\n",
    "\n",
    "    # Perform prediction\n",
    "    predictions = prediction_fn(inputs=landmark)\n",
    "\n",
    "    # Get the predicted sign\n",
    "    sign = predictions['outputs'].argmax()\n",
    "\n",
    "    # Print the predicted sign (modify this part according to your requirements)\n",
    "    print(f\"Predicted sign: {sign}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Create a new asyncio event loop and run the capture task\n",
    "        loop = asyncio.new_event_loop()\n",
    "        asyncio.set_event_loop(loop)\n",
    "        loop.run_until_complete(do_capture_web_async())\n",
    "    except KeyboardInterrupt:\n",
    "        # If Esc is pressed, stop the capture task and exit gracefully\n",
    "        print(\"Process interrupted by Esc key.\")\n",
    "    finally:\n",
    "        # Release any resources (e.g., close the model, etc.) before exiting\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09a52ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b260e128",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aecb286a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_holistic = mp.solutions.holistic\n",
    "\n",
    "\n",
    "\n",
    "def create_landmark_frame_df(results,frame,xyz):\n",
    "    \"\"\"\n",
    "    xyz takes the results from mediapipe and creates a dataframe of the landmark\n",
    "    \n",
    "    inputs:\n",
    "        results: mediapipe results object\n",
    "        frame: frame number\n",
    "        xyz: dataframe wof the xyz example data\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #we want the values and rows for every type landmark index so we need skeleton\n",
    "    xyz_skel = xyz[['type','landmark_index']].drop_duplicates() \\\n",
    "    .reset_index(drop=True).copy()\n",
    "    \n",
    "    pose = pd.DataFrame()\n",
    "    face = pd.DataFrame()\n",
    "    left_hand = pd.DataFrame()\n",
    "    right_hand = pd.DataFrame()\n",
    "    \n",
    "    if results.face_landmarks is not None:\n",
    "        for i, point in enumerate(results.face_landmarks.landmark):\n",
    "            face.loc[i, ['x','y','z']] = [point.x, point.y, point.z]\n",
    "\n",
    "    if results.pose_landmarks is not None:\n",
    "        for i, point in enumerate(results.pose_landmarks.landmark):\n",
    "            pose.loc[i, ['x','y','z']] = [point.x, point.y, point.z]\n",
    "\n",
    "\n",
    "    if results.left_hand_landmarks is not None:\n",
    "        for i, point in enumerate(results.left_hand_landmarks.landmark):\n",
    "            left_hand.loc[i, ['x','y','z']] = [point.x, point.y, point.z]\n",
    "\n",
    "\n",
    "    if results.right_hand_landmarks is not None:\n",
    "        for i, point in enumerate(results.right_hand_landmarks.landmark):\n",
    "            right_hand.loc[i, ['x','y','z']] = [point.x, point.y, point.z]\n",
    "\n",
    "    face = face.reset_index() \\\n",
    "    .rename(columns={'index': 'landmark_index'}) \\\n",
    "    .assign(type='face')\n",
    "\n",
    "    pose = pose.reset_index() \\\n",
    "    .rename(columns={'index': 'landmark_index'}) \\\n",
    "    .assign(type='pose')\n",
    "\n",
    "    left_hand = left_hand.reset_index() \\\n",
    "    .rename(columns={'index': 'landmark_index'}) \\\n",
    "    .assign(type='left_hand')\n",
    "\n",
    "    right_hand = right_hand.reset_index() \\\n",
    "    .rename(columns={'index': 'landmark_index'}) \\\n",
    "    .assign(type='right_hand')\n",
    "\n",
    "    landmark = pd.concat([face,pose,left_hand,right_hand]).reset_index(drop=True)\n",
    "    #merge with lanndmark\n",
    "    landmark = xyz_skel.merge(landmark, on=['type','landmark_index'], how='left')\n",
    "    #assign frames to make it unique\n",
    "    landmark = landmark.assign(frame = frame)\n",
    "    return landmark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65ef005c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "ROWS_PER_FRAME = 543\n",
    "\n",
    "def load_relevent_data_subset(pq_path):\n",
    "    data_columns = ['x','y','z']\n",
    "    data = pd.read_parquet(pq_path,columns=data_columns)\n",
    "    n_frames = int(len(data) / ROWS_PER_FRAME)\n",
    "    data = data.values.reshape(n_frames, ROWS_PER_FRAME,len(data_columns))\n",
    "    return data.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9357c7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "tensorflow/lite/kernels/reduce.cc:135 current >= 0 && current < input_num_dims was not true.Node number 5 (SUM) failed to prepare.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 85\u001b[0m\n\u001b[1;32m     83\u001b[0m prediction_fn \u001b[38;5;241m=\u001b[39m interpreter\u001b[38;5;241m.\u001b[39mget_signature_runner(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserving_default\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m#pass to prediction_fn\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mprediction_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlandmark\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m#get the maximum output value\u001b[39;00m\n\u001b[1;32m     88\u001b[0m sign \u001b[38;5;241m=\u001b[39m predictions[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39margmax()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:243\u001b[0m, in \u001b[0;36mSignatureRunner.__call__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpreter_wrapper\u001b[38;5;241m.\u001b[39mResizeInputTensor(\n\u001b[1;32m    240\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inputs[input_name], np\u001b[38;5;241m.\u001b[39marray(value\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint32),\n\u001b[1;32m    241\u001b[0m       \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_subgraph_index)\n\u001b[1;32m    242\u001b[0m \u001b[38;5;66;03m# Allocate tensors.\u001b[39;00m\n\u001b[0;32m--> 243\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpreter_wrapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAllocateTensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_subgraph_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;66;03m# Set the input values.\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m input_name, value \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems():\n",
      "\u001b[0;31mRuntimeError\u001b[0m: tensorflow/lite/kernels/reduce.cc:135 current >= 0 && current < input_num_dims was not true.Node number 5 (SUM) failed to prepare."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "import asyncio\n",
    "import pandas as pd\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_holistic = mp.solutions.holistic\n",
    "\n",
    "all_landmarks = []\n",
    "\n",
    "# For webcam input:\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5,min_tracking_confidence=0.5) as holistic:\n",
    "    #initially frame is set to 0\n",
    "    frame = 0 \n",
    "    while cap.isOpened():\n",
    "        #increment the frame by 1\n",
    "        frame+=1\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "              print(\"Ignoring empty camera frame.\")\n",
    "              # If loading a video, use 'break' instead of 'continue'.\n",
    "              continue\n",
    "\n",
    "        # To improve performance, optionally mark the image as not writeable to\n",
    "        # pass by reference.\n",
    "        image.flags.writeable = False\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = holistic.process(image)\n",
    "        \n",
    "        #passing the example dataset\n",
    "        xyz = pd.read_parquet('./2044/635217.parquet')\n",
    "        \n",
    "        #create landmark dataframe\n",
    "        \n",
    "        landmark = create_landmark_frame_df(results, frame, xyz)\n",
    "        all_landmarks.append(landmark)\n",
    "\n",
    "        # Draw landmark annotation on the image.\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        mp_drawing.draw_landmarks(\n",
    "            image,\n",
    "            results.face_landmarks,\n",
    "            mp_holistic.FACEMESH_CONTOURS,\n",
    "            landmark_drawing_spec=None,\n",
    "            connection_drawing_spec=mp_drawing_styles\n",
    "            .get_default_face_mesh_contours_style())\n",
    "    \n",
    "        mp_drawing.draw_landmarks(\n",
    "            image,\n",
    "            results.pose_landmarks,\n",
    "            mp_holistic.POSE_CONNECTIONS,\n",
    "            landmark_drawing_spec=mp_drawing_styles\n",
    "            .get_default_pose_landmarks_style())\n",
    "\n",
    "        mp_drawing.draw_landmarks(\n",
    "                image,\n",
    "                results.left_hand_landmarks,\n",
    "                mp_holistic.HAND_CONNECTIONS,\n",
    "                landmark_drawing_spec=mp_drawing_styles\n",
    "                .get_default_hand_landmarks_style())\n",
    "\n",
    "        mp_drawing.draw_landmarks(\n",
    "                image,\n",
    "                results.right_hand_landmarks,\n",
    "                mp_holistic.HAND_CONNECTIONS,\n",
    "                landmark_drawing_spec=mp_drawing_styles\n",
    "                .get_default_hand_landmarks_style())\n",
    "        \n",
    "        landmark = pd.concat(all_landmarks).reset_index(drop=True)\n",
    "        \n",
    "        #load the model\n",
    "        interpreter = tf.lite.Interpreter(\"./model.tflite\")\n",
    "        \n",
    "        #interpret the model\n",
    "        interpreter = tf.lite.Interpreter(model_path=\"./model.tflite\")\n",
    "\n",
    "        found_signatures = list(interpreter.get_signature_list().keys())\n",
    "        \n",
    "        prediction_fn = interpreter.get_signature_runner(\"serving_default\")\n",
    "        #pass to prediction_fn\n",
    "        predictions = prediction_fn(inputs=landmark)\n",
    "        \n",
    "        #get the maximum output value\n",
    "        sign = predictions['outputs'].argmax()\n",
    "        \n",
    "        #encode\n",
    "        train = pd.read_csv('train.csv')\n",
    "\n",
    "        # Add ordinally Encoded Sign (assign number to each sign name)\n",
    "        train['sign_ord'] = train['sign'].astype('category').cat.codes\n",
    "\n",
    "        # Dictionaries to translate sign <-> ordinal encoded sign\n",
    "        SIGN2ORD = train[['sign', 'sign_ord']].set_index('sign').squeeze().to_dict()\n",
    "        ORD2SIGN = train[['sign_ord', 'sign']].set_index('sign_ord').squeeze().to_dict()\n",
    "        predict_value = ORD2SIGN[sign]\n",
    "        print(predict_value)\n",
    "\n",
    "        # Flip the image horizontally for a selfie-view display.\n",
    "        cv2.imshow('MediaPipe Holistic', cv2.flip(image, 1))\n",
    "        if cv2.waitKey(5) & 0xFF == 27:\n",
    "            break\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3533289c",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m     row_index \u001b[38;5;241m=\u001b[39m i \u001b[38;5;241m%\u001b[39m ROWS_PER_FRAME\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j, column \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data_columns):\n\u001b[0;32m---> 27\u001b[0m         reshaped_data[frame_index, row_index, j] \u001b[38;5;241m=\u001b[39m entry[column]\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# The reshaped_data array now contains the data in the desired format\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(reshaped_data)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sample data (list of dictionaries)\n",
    "data = [\n",
    "    {'type': 'face', 'landmark_index': 0, 'x': 0.526375, 'y': 0.678677, 'z': -0.034196, 'frame': 1},\n",
    "    {'type': 'face', 'landmark_index': 1, 'x': 0.523675, 'y': 0.623428, 'z': -0.057393, 'frame': 1},\n",
    "    # ... (more data)\n",
    "]\n",
    "\n",
    "# Define the number of rows per frame\n",
    "ROWS_PER_FRAME = 543\n",
    "\n",
    "# Define the data columns\n",
    "data_columns = ['x', 'y', 'z']\n",
    "\n",
    "# Calculate the number of frames\n",
    "n_frames = len(data) // ROWS_PER_FRAME\n",
    "\n",
    "# Create an empty array to hold the reshaped data\n",
    "reshaped_data = np.empty((n_frames, ROWS_PER_FRAME, len(data_columns)), dtype=np.float32)\n",
    "\n",
    "# Iterate through the original data and fill in the reshaped_data array\n",
    "for i, entry in enumerate(data):\n",
    "    frame_index = i // ROWS_PER_FRAME\n",
    "    row_index = i % ROWS_PER_FRAME\n",
    "    for j, column in enumerate(data_columns):\n",
    "        reshaped_data[frame_index, row_index, j] = entry[column]\n",
    "\n",
    "# The reshaped_data array now contains the data in the desired format\n",
    "print(reshaped_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09b93de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'face', 'landmark_index': 0, 'x': 0.526375, 'y': 0.678677, 'z': -0.034196, 'frame': 1}\n",
      "{'type': 'face', 'landmark_index': 1, 'x': 0.523675, 'y': 0.623428, 'z': -0.057393, 'frame': 1}\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    {'type': 'face', 'landmark_index': 0, 'x': 0.526375, 'y': 0.678677, 'z': -0.034196, 'frame': 1},\n",
    "    {'type': 'face', 'landmark_index': 1, 'x': 0.523675, 'y': 0.623428, 'z': -0.057393, 'frame': 1},\n",
    "    # ... (more data)\n",
    "]\n",
    "for i, entry in enumerate(data):\n",
    "   print(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0bf42ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.526375  0.678677 -0.034196]\n",
      "  [ 0.523675  0.623428 -0.057393]\n",
      "  [ 0.        0.        0.      ]\n",
      "  ...\n",
      "  [ 0.        0.        0.      ]\n",
      "  [ 0.        0.        0.      ]\n",
      "  [ 0.        0.        0.      ]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = [\n",
    "    {'type': 'face', 'landmark_index': 0, 'x': 0.526375, 'y': 0.678677, 'z': -0.034196, 'frame': 1},\n",
    "    {'type': 'face', 'landmark_index': 1, 'x': 0.523675, 'y': 0.623428, 'z': -0.057393, 'frame': 1},\n",
    "    # ... (more data)\n",
    "]\n",
    "\n",
    "data_columns = ['x', 'y', 'z']\n",
    "ROWS_PER_FRAME = 543\n",
    "\n",
    "# Calculate the number of frames based on the number of rows per frame and the data length\n",
    "n_frames = (len(data) + ROWS_PER_FRAME - 1) // ROWS_PER_FRAME\n",
    "\n",
    "# Initialize the reshaped_data array with zeros\n",
    "reshaped_data = np.zeros((n_frames, ROWS_PER_FRAME, len(data_columns)), dtype=np.float32)\n",
    "\n",
    "# Iterate through the data and populate the reshaped_data array\n",
    "for i, entry in enumerate(data):\n",
    "    frame_index = i // ROWS_PER_FRAME\n",
    "    row_index = i % ROWS_PER_FRAME\n",
    "    for j, column in enumerate(data_columns):\n",
    "        reshaped_data[frame_index, row_index, j] = entry[column]\n",
    "\n",
    "# Now reshaped_data contains the data in the desired shape\n",
    "print(reshaped_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3af91f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 5.263750e-01  6.786770e-01 -3.419600e-02]\n",
      "  [ 5.236750e-01  6.234280e-01 -5.739300e-02]\n",
      "  [          nan           nan           nan]\n",
      "  ...\n",
      "  [ 8.263918e-37  4.573278e-41  7.566771e-37]\n",
      "  [ 4.573278e-41  7.565394e-37  4.573278e-41]\n",
      "  [ 7.160148e-37  4.573278e-41  9.199434e-37]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    {'type': 'face', 'landmark_index': 0, 'x': 0.526375, 'y': 0.678677, 'z': -0.034196, 'frame': 1},\n",
    "    {'type': 'face', 'landmark_index': 1, 'x': 0.523675, 'y': 0.623428, 'z': -0.057393, 'frame': 1},\n",
    "    # ... (more data)\n",
    "    {'type': 'right_hand', 'landmark_index': 16, 'x': None, 'y': None, 'z': None, 'frame': 24},\n",
    "    {'type': 'right_hand', 'landmark_index': 17, 'x': None, 'y': None, 'z': None, 'frame': 24},\n",
    "    # ... (more data)\n",
    "]\n",
    "\n",
    "data_columns = ['x', 'y', 'z']\n",
    "ROWS_PER_FRAME = 543\n",
    "\n",
    "# Calculate the number of frames based on the number of rows per frame and the data length\n",
    "n_frames = (len(data) + ROWS_PER_FRAME - 1) // ROWS_PER_FRAME\n",
    "\n",
    "# Initialize the reshaped_data array with zeros\n",
    "reshaped_data = np.empty((n_frames, ROWS_PER_FRAME, len(data_columns)), dtype=np.float32)\n",
    "\n",
    "# Iterate through the data and populate the reshaped_data array\n",
    "for i, entry in enumerate(data):\n",
    "    frame_index = i // ROWS_PER_FRAME\n",
    "    row_index = i % ROWS_PER_FRAME\n",
    "    if all(entry[column] is None for column in data_columns):\n",
    "        reshaped_data[frame_index, row_index] = np.nan\n",
    "    else:\n",
    "        for j, column in enumerate(data_columns):\n",
    "            reshaped_data[frame_index, row_index, j] = entry[column]\n",
    "\n",
    "# Now reshaped_data contains the data in the desired shape, including NaN values\n",
    "print(reshaped_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d4b4336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.526375  0.678677 -0.034196]\n",
      "  [ 0.523675  0.623428 -0.057393]\n",
      "  [      nan       nan       nan]\n",
      "  ...\n",
      "  [      nan       nan       nan]\n",
      "  [      nan       nan       nan]\n",
      "  [      nan       nan       nan]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = [\n",
    "    {'type': 'face', 'landmark_index': 0, 'x': 0.526375, 'y': 0.678677, 'z': -0.034196, 'frame': 1},\n",
    "    {'type': 'face', 'landmark_index': 1, 'x': 0.523675, 'y': 0.623428, 'z': -0.057393, 'frame': 1},\n",
    "    # ... (more data)\n",
    "    {'type': 'right_hand', 'landmark_index': 16, 'x': None, 'y': None, 'z': None, 'frame': 24},\n",
    "    {'type': 'right_hand', 'landmark_index': 17, 'x': None, 'y': None, 'z': None, 'frame': 24},\n",
    "    # ... (more data)\n",
    "]\n",
    "\n",
    "data_columns = ['x', 'y', 'z']\n",
    "ROWS_PER_FRAME = 543\n",
    "\n",
    "# Calculate the number of frames based on the number of rows per frame and the data length\n",
    "n_frames = (len(data) + ROWS_PER_FRAME - 1) // ROWS_PER_FRAME\n",
    "\n",
    "# Initialize the reshaped_data array with NaN values\n",
    "reshaped_data = np.empty((n_frames, ROWS_PER_FRAME, len(data_columns)), dtype=np.float32)\n",
    "reshaped_data[:] = np.nan\n",
    "\n",
    "# Iterate through the data and populate the reshaped_data array\n",
    "for i, entry in enumerate(data):\n",
    "    frame_index = i // ROWS_PER_FRAME\n",
    "    row_index = i % ROWS_PER_FRAME\n",
    "    for j, column in enumerate(data_columns):\n",
    "        if entry[column] is not None:\n",
    "            reshaped_data[frame_index, row_index, j] = entry[column]\n",
    "\n",
    "# Now reshaped_data contains the data in the desired shape, including NaN values\n",
    "print(reshaped_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "115c2715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.553937  0.533765 -0.012798]\n",
      "  [ 0.549591  0.488464 -0.061401]\n",
      "  [      nan       nan       nan]\n",
      "  ...\n",
      "  [      nan       nan       nan]\n",
      "  [      nan       nan       nan]\n",
      "  [      nan       nan       nan]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data (based on your provided data)\n",
    "data = [\n",
    "    {'type': 'face', 'landmark_index': 0, 'x': 0.553937, 'y': 0.533765, 'z': -0.012798, 'frame': 1},\n",
    "    {'type': 'face', 'landmark_index': 1, 'x': 0.549591, 'y': 0.488464, 'z': -0.061401, 'frame': 1},\n",
    "    # ... (more data)\n",
    "    {'type': 'right_hand', 'landmark_index': 16, 'x': None, 'y': None, 'z': None, 'frame': 24},\n",
    "    {'type': 'right_hand', 'landmark_index': 17, 'x': None, 'y': None, 'z': None, 'frame': 24},\n",
    "    # ... (more data)\n",
    "]\n",
    "\n",
    "data_columns = ['x', 'y', 'z']\n",
    "ROWS_PER_FRAME = 543\n",
    "\n",
    "# Create a DataFrame from the data list\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate the number of frames based on the number of rows per frame\n",
    "n_frames = (len(df) + ROWS_PER_FRAME - 1) // ROWS_PER_FRAME\n",
    "\n",
    "# Initialize the reshaped_data array with NaN values\n",
    "reshaped_data = np.empty((n_frames, ROWS_PER_FRAME, len(data_columns)), dtype=np.float32)\n",
    "reshaped_data[:] = np.nan\n",
    "\n",
    "# Iterate through the data and populate the reshaped_data array\n",
    "for i, entry in df.iterrows():\n",
    "    frame_index = i // ROWS_PER_FRAME\n",
    "    row_index = i % ROWS_PER_FRAME\n",
    "    for j, column in enumerate(data_columns):\n",
    "        if pd.notna(entry[column]):\n",
    "            reshaped_data[frame_index, row_index, j] = entry[column]\n",
    "\n",
    "# Now reshaped_data contains the data in the desired shape, including NaN values\n",
    "print(reshaped_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5bebde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
