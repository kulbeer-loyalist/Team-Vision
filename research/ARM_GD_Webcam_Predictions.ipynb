{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARM - Webcam Predictions\n",
    "\n",
    "This script captures the image from the webam, preprocess it, and then predicts the sign using the latest model.\n",
    "\n",
    "Created by:\n",
    "\n",
    "- Marcus Vinicius da Silva Fernandes.\n",
    "\n",
    "2023-08-12.\n",
    "\n",
    "ARM = Action Recognition Modelling.\n",
    "\n",
    "References:\n",
    "- https://www.youtube.com/watch?v=doDUihpj6ro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keytotext -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up of the Holistic model by Mediapipe\n",
    "\n",
    "It will run the following models:\n",
    "- pose_landmarks\n",
    "- face_landmarks\n",
    "- left_hand_landmarks\n",
    "- right_hand_landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic  # for landmarks detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Landmarks detection function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to detect the landmarks in each frame or image\n",
    "def landmark_detection(frame, model):\n",
    "    # Color conversion because mediapipe's landmark detection model expects RGB frames as input.\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # color conversion BGR to RGB.\n",
    "    frame.flags.writeable = False  # frame is not writeable.\n",
    "    results = model.process(frame)  # landmarks detection.\n",
    "    frame.flags.writeable = True  # frame is writeable.\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)  # color conversion RGB to BGR.\n",
    "    return frame, results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Landmarks coordinates extraction function\n",
    "\n",
    "It will :\n",
    "- Extract the coordinates from the parameter 'results'.\n",
    "- Only x and y coordinates are used.\n",
    "- Store them into a numpy array.\n",
    "- It will store zeros if the parameter 'results' has no value for the model (e.g. it can happen when the hand was not visible and therefore was not identified).\n",
    "- The variables lh_visible and rh_visible will indicate if the hands are visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract the coordinates of the detected landmarks\n",
    "def landmark_extraction(results):\n",
    "    lh_visible = 0\n",
    "    rh_visible = 0\n",
    "\n",
    "    if results.face_landmarks:\n",
    "        face = np.array([[coordinate.x, coordinate.y] for coordinate in results.face_landmarks.landmark])\n",
    "    else:\n",
    "        face = np.array([[0, 0] for idx in range(468)])\n",
    "\n",
    "    if results.left_hand_landmarks:\n",
    "        left_hand = np.array([[coordinate.x, coordinate.y] for coordinate in results.left_hand_landmarks.landmark])\n",
    "        lh_visible = 1\n",
    "    else:\n",
    "        left_hand = np.array([[0, 0] for idx in range(21)])\n",
    "        lh_visible = 0\n",
    "        \n",
    "    if results.pose_landmarks:\n",
    "        pose = np.array([[coordinate.x, coordinate.y] for coordinate in results.pose_landmarks.landmark])\n",
    "    else:\n",
    "        pose = np.array([[0, 0] for idx in range(33)])\n",
    "    \n",
    "    if results.right_hand_landmarks:\n",
    "        right_hand = np.array([[coordinate.x, coordinate.y] for coordinate in results.right_hand_landmarks.landmark])\n",
    "        rh_visible = 1\n",
    "    else:\n",
    "        right_hand = np.array([[0, 0] for idx in range(21)])\n",
    "        rh_visible = 0\n",
    "            \n",
    "    return np.concatenate([face, left_hand, pose, right_hand]), lh_visible, rh_visible\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and shaping the landmarks to the desired number of frames\n",
    "\n",
    "- Creation of a dictionary to associate the words to a unique number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the json file adn creation of dictionary to associate the words to a unique number\n",
    "with open('sign_to_prediction_index_map.json', 'r') as j:\n",
    "     sign_dict = json.loads(j.read())\n",
    "\n",
    "del j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desired number of frames\n",
    "- Each video will be reshaped to have the number of rows (or frames) equal to the desired number of frames defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FRAMES = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Landmark points to keep\n",
    "- The objective is to reduce the number of features.\n",
    "- All the landmarks from the hands will be kept.\n",
    "- Other landmarks to keep: outline of mouth from face mesh model, and arms, shoulders, and face from pose model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_keep_points = [0, 267, 269, 270, 409, 291, 375, 321, 405, 314, 17, 84, 181, 91, 146, 61, 185, 40, 39, 37]\n",
    "face_keep_points.sort()\n",
    "left_hand_keep_points = [i for i in range(21)]\n",
    "pose_keep_points = [0, 1, 2, 3, 4, 5, 6, 7, 8, 11, 12, 13, 14, 15, 16, 23, 24]\n",
    "right_hand_keep_points = [i for i in range(21)]\n",
    "\n",
    "face_keep_idx = [face_keep_points[i] for i in range(len(face_keep_points))]\n",
    "left_hand_keep_idx = [i + 468 for i in left_hand_keep_points]\n",
    "pose_keep_idx = [i + 468 + 21 for i in pose_keep_points]\n",
    "right_hand_keep_idx = [i + 468 + 21 + 33 for i in right_hand_keep_points]\n",
    "\n",
    "landmarks_to_keep = face_keep_idx + left_hand_keep_idx + pose_keep_idx + right_hand_keep_idx\n",
    "\n",
    "del face_keep_points, left_hand_keep_points, pose_keep_points, right_hand_keep_points\n",
    "del face_keep_idx, left_hand_keep_idx, pose_keep_idx, right_hand_keep_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_ROWS = 543\n",
    "desired_num_rows = len(landmarks_to_keep) * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_landmarks(data):\n",
    "\n",
    "    landmarks = np.empty((1, NUM_FRAMES, desired_num_rows), dtype=float)\n",
    " \n",
    "    # Reshaping the data\n",
    "    num_frames = int(len(data) / TOTAL_ROWS)\n",
    "    data = data.reshape(num_frames, TOTAL_ROWS, 2)\n",
    "    data.astype(np.float32)\n",
    "\n",
    "    # Dropping undesired points\n",
    "    data = data[:, landmarks_to_keep]\n",
    "\n",
    "    # Adjusting the number of frames\n",
    "    if data.shape[0] > NUM_FRAMES:  # time-based sampling\n",
    "        indices = np.arange(0, data.shape[0], data.shape[0] // NUM_FRAMES)[:NUM_FRAMES]\n",
    "        data = data[indices]\n",
    "    elif data.shape[0] < NUM_FRAMES:  # padding the videos\n",
    "        rows = NUM_FRAMES - data.shape[0]\n",
    "        data = np.append(np.zeros((rows, len(landmarks_to_keep), 2)), data, axis=0)\n",
    "\n",
    "    # Reshaping the data\n",
    "    landmarks = data.reshape(NUM_FRAMES, len(landmarks_to_keep) * 2, order='F')\n",
    "    del data\n",
    "\n",
    "    return landmarks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP function\n",
    "\n",
    "To generate the sentences from the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keytotext import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_generation(keywords,model_list):\n",
    "    # models=[\"k2t\",\"k2t-base\",\"mrm8488/t5-base-finetuned-common_gen\"]\n",
    "    models_dict = {'k2t_model_tuned':\"mrm8488/t5-base-finetuned-common_gen\"}\n",
    "    model = pipeline(models_dict['k2t_model_tuned'])\n",
    "    model_list.append(model(keywords))\n",
    "    return model_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " masking_1 (Masking)         (None, None, 158)         0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, None, 512)         81408     \n",
      "                                                                 \n",
      " layer_normalization_2 (Lay  (None, None, 512)         1024      \n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, None, 512)         0         \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, None, 512)         0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, None, 256)         131328    \n",
      "                                                                 \n",
      " layer_normalization_3 (Lay  (None, None, 256)         512       \n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, None, 256)         0         \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, None, 256)         0         \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, None, 256)         525312    \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, None, 256)         0         \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 256)               525312    \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 250)               64250     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1394938 (5.32 MB)\n",
      "Trainable params: 1394938 (5.32 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LayerNormalization, Activation, Dropout, LSTM, Masking\n",
    "\n",
    "input_shape = (None, 158)\n",
    "output_classes = 250\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Masking(mask_value=0, input_shape=input_shape))\n",
    "\n",
    "model.add(Dense(512))\n",
    "model.add(LayerNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(256))\n",
    "model.add(LayerNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Dense(256, activation='relu'))\n",
    "\n",
    "model.add(Dense(output_classes, activation='softmax'))\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('08-12_ARM_GD_Final-Architecture.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main code for detection and extraction\n",
    "- Capturing the image from the webcam and converting it into frames by OpenCV.\n",
    "- For each frame, the function landmark_detection will be called to make the detections.\n",
    "- If at least one hand is visible, the landmarks will be stored in landmarks_array. Otherwise, they will be discarded.\n",
    "- When the performer hides the hands from the camera after performing the sign, the array containing the landmarks will be preprocessed and then a prediction will happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n",
      "7 ['any']\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "127 ['jump']\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "194 ['shhh']\n",
      "Sentence:  [\"shhh... i don't know if i can jump on any of these.\"]\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "127 ['jump']\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "77 ['feet']\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "120 ['hot']\n",
      "Sentence:  ['A man is jumping on a hot dog with his feet down.']\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "205 ['stay']\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "196 ['shoe']\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "196 ['shoe']\n",
      "Sentence:  ['a pair of shoes that are made to stay on the ground']\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "205 ['stay']\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "10 ['aunt']\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "147 ['morning']\n",
      "Sentence:  ['A little girl is staying with her aunt in the morning.']\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "225 ['touch']\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "61 ['down']\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "236 ['weus']\n",
      "Sentence:  ['a man touches down a weus.']\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "196 ['shoe']\n"
     ]
    }
   ],
   "source": [
    "# Capturing the video frames from the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# List that will receive the landmark's coordinates for each video\n",
    "landmarks_list = []\n",
    "\n",
    "sign_status = 0  # 0 = not performing a sign / 1 = performing a sign\n",
    "\n",
    "words_for_nlp = np.empty((0))\n",
    "\n",
    "# Set mediapipe model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "                \n",
    "    # Looping through all the frames\n",
    "    while cap.isOpened():  # making sure it is reading frames\n",
    "\n",
    "        # Reading the frames\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Making detections\n",
    "        image, results = landmark_detection(frame, holistic)\n",
    "\n",
    "        # Draw landmarks\n",
    "\n",
    "        # Show to screen\n",
    "        cv2.imshow(\"Video\", image)\n",
    "                \n",
    "        # Extracting landmarks\n",
    "        landmarks_list_np, lh_visible, rh_visible = landmark_extraction(results)\n",
    "        landmarks_list.append(landmarks_list_np)\n",
    "\n",
    "        # Storing the landmarks in an array when the sign is being performed (at least one hand is visible).\n",
    "        if lh_visible == 1 or rh_visible == 1:\n",
    "            landmarks_array = np.concatenate(landmarks_list, axis=0)\n",
    "            sign_status = 1\n",
    "        \n",
    "        # Making predictions when the sign is done (both hands are not visible).\n",
    "        if lh_visible == 0 and rh_visible == 0 and sign_status == 1:\n",
    "            # Shaping the array\n",
    "            x_test = np.expand_dims(preprocess_landmarks(landmarks_array), axis=0)\n",
    "\n",
    "            # Making predictions\n",
    "            predicted_label = np.argmax(model.predict(x_test))\n",
    "            predicted_word = np.array([list(sign_dict.keys())[list(sign_dict.values()).index(predicted_label)]])\n",
    "            print(predicted_label, predicted_word)\n",
    "\n",
    "            # Storing the words in an array, to send to text-to-text model\n",
    "            words_for_nlp = np.append(words_for_nlp, predicted_word)\n",
    "            \n",
    "            # Reseting the variables\n",
    "            landmarks_list = []\n",
    "            sign_status = 0\n",
    "            del landmarks_list_np, landmarks_array\n",
    "\n",
    "        # Getting the sentence from the predicted words\n",
    "        if len(words_for_nlp) == 3:\n",
    "            model_list = []\n",
    "            model_list = sentence_generation(words_for_nlp, model_list)\n",
    "            print('Sentence: ', model_list)\n",
    "            words_for_nlp = np.empty((0))\n",
    "\n",
    "        # Break gracefully (it will close capturing video from webcam when user press the button Q)\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
