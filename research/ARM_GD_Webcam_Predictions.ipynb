{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webcam Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up of the Holistic model by Mediapipe\n",
    "\n",
    "It will run the following models:\n",
    "- pose_landmarks\n",
    "- face_landmarks\n",
    "- left_hand_landmarks\n",
    "- right_hand_landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic  # for landmarks detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Landmarks detection function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to detect the landmarks in each frame or image\n",
    "def landmark_detection(frame, model):\n",
    "    # Color conversion because mediapipe's landmark detection model expects RGB frames as input.\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # color conversion BGR to RGB.\n",
    "    frame.flags.writeable = False  # frame is not writeable.\n",
    "    results = model.process(frame)  # landmarks detection.\n",
    "    frame.flags.writeable = True  # frame is writeable.\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)  # color conversion RGB to BGR.\n",
    "    return frame, results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Landmarks coordinates extraction function\n",
    "\n",
    "It will :\n",
    "- Extract the coordinates from the parameter 'results'.\n",
    "- Only x and y coordinates are saved\n",
    "- Store them into a numpy array.\n",
    "    - 'flatten' function will write all the coordinates in a single array, so the length will be:\n",
    "        - Pose: 2 coordinates x 33 landmarks = 66 values.\n",
    "        - Left hand: 2 coordinates x 21 landmarks = 42 values.\n",
    "        - Right hand: 2 coordinates x 21 landmarks = 42 values.\n",
    "        - Face: 2 coordinates x 468 landmarks = 936 values.\n",
    "        - Each row (each frame) will have a total of 1086 values after concatenation.\n",
    "    - It will store zeros if the parameter 'results' has no value for the model (e.g. it can happen when the hand was not visible and therefore was not identified)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract the coordinates of the detected landmarks\n",
    "def landmark_extraction(results):\n",
    "    lh_visible = 0\n",
    "    rh_visible = 0\n",
    "\n",
    "    if results.face_landmarks:\n",
    "        face = np.array([[coordinate.x, coordinate.y] for coordinate in results.face_landmarks.landmark])\n",
    "    else:\n",
    "        face = np.array([[0, 0] for idx in range(468)])\n",
    "\n",
    "    if results.left_hand_landmarks:\n",
    "        left_hand = np.array([[coordinate.x, coordinate.y] for coordinate in results.left_hand_landmarks.landmark])\n",
    "        lh_visible = 1\n",
    "    else:\n",
    "        left_hand = np.array([[0, 0] for idx in range(21)])\n",
    "        lh_visible = 0\n",
    "        \n",
    "    if results.pose_landmarks:\n",
    "        pose = np.array([[coordinate.x, coordinate.y] for coordinate in results.pose_landmarks.landmark])\n",
    "    else:\n",
    "        pose = np.array([[0, 0] for idx in range(33)])\n",
    "    \n",
    "    if results.right_hand_landmarks:\n",
    "        right_hand = np.array([[coordinate.x, coordinate.y] for coordinate in results.right_hand_landmarks.landmark])\n",
    "        rh_visible = 1\n",
    "    else:\n",
    "        right_hand = np.array([[0, 0] for idx in range(21)])\n",
    "        rh_visible = 0\n",
    "            \n",
    "    return np.concatenate([face, left_hand, pose, right_hand]), lh_visible, rh_visible\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and shaping the landmarks to the desired number of frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creation of a dictionary to associate the words to a unique number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the json file adn creation of dictionary to associate the words to a unique number\n",
    "with open('sign_to_prediction_index_map.json', 'r') as j:\n",
    "     sign_dict = json.loads(j.read())\n",
    "\n",
    "del j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desired number of frames\n",
    "- Each video will be reshaped to have the number of rows (or frames) equal to the desired number of frames defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FRAMES = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Landmark points to keep\n",
    "- The objective is to reduce the number of features.\n",
    "- All the landmarks from the hands will be kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_keep_points = [0, 267, 269, 270, 409, 291, 375, 321, 405, 314, 17, 84, 181, 91, 146, 61, 185, 40, 39, 37]\n",
    "face_keep_points.sort()\n",
    "left_hand_keep_points = [i for i in range(21)]\n",
    "pose_keep_points = [0, 1, 2, 3, 4, 5, 6, 7, 8, 11, 12, 13, 14, 15, 16, 23, 24]\n",
    "right_hand_keep_points = [i for i in range(21)]\n",
    "\n",
    "face_keep_idx = [face_keep_points[i] for i in range(len(face_keep_points))]\n",
    "left_hand_keep_idx = [i + 468 for i in left_hand_keep_points]\n",
    "pose_keep_idx = [i + 468 + 21 for i in pose_keep_points]\n",
    "right_hand_keep_idx = [i + 468 + 21 + 33 for i in right_hand_keep_points]\n",
    "\n",
    "landmarks_to_keep = face_keep_idx + left_hand_keep_idx + pose_keep_idx + right_hand_keep_idx\n",
    "\n",
    "del face_keep_points, left_hand_keep_points, pose_keep_points, right_hand_keep_points\n",
    "del face_keep_idx, left_hand_keep_idx, pose_keep_idx, right_hand_keep_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_ROWS = 543\n",
    "desired_num_rows = len(landmarks_to_keep) * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_landmarks(data):\n",
    "\n",
    "    landmarks = np.empty((1, NUM_FRAMES, desired_num_rows), dtype=float)\n",
    " \n",
    "    # Reshaping the data\n",
    "    num_frames = int(len(data) / TOTAL_ROWS)\n",
    "    data = data.reshape(num_frames, TOTAL_ROWS, 2)\n",
    "    data.astype(np.float32)\n",
    "\n",
    "    # Dropping undesired points\n",
    "    data = data[:, landmarks_to_keep]\n",
    "\n",
    "    # Adjusting the number of frames\n",
    "    if data.shape[0] > NUM_FRAMES:  # time-based sampling\n",
    "        indices = np.arange(0, data.shape[0], data.shape[0] // NUM_FRAMES)[:NUM_FRAMES]\n",
    "        data = data[indices]\n",
    "    elif data.shape[0] < NUM_FRAMES:  # padding the videos\n",
    "        rows = NUM_FRAMES - data.shape[0]\n",
    "        data = np.append(np.zeros((rows, len(landmarks_to_keep), 2)), data, axis=0)\n",
    "\n",
    "    # Reshaping the data\n",
    "    landmarks = data.reshape(NUM_FRAMES, len(landmarks_to_keep) * 2, order='F')\n",
    "    del data\n",
    "\n",
    "    return landmarks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LayerNormalization, Activation, Dropout, LSTM, Masking\n",
    "\n",
    "input_shape = (None, 158)\n",
    "output_classes = 250\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Masking(mask_value=0, input_shape=input_shape))\n",
    "\n",
    "model.add(Dense(512))\n",
    "model.add(LayerNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(256))\n",
    "model.add(LayerNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Dense(256, activation='relu'))\n",
    "\n",
    "model.add(Dense(output_classes, activation='softmax'))\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('08-12_ARM_GD_Final-Architecture.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main code for detection and extraction\n",
    "- Loading the videos and converting them into frames by OpenCV.\n",
    "- For each frame, the function landmark_detection will be called to make the detections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capturing the video frames from the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# List that will receive the landmark's coordinates for each video\n",
    "landmarks_list = []\n",
    "sign_status = 0  # 0 = not performing a sign / 1 = performing a sign\n",
    "\n",
    "# Set mediapipe model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "                \n",
    "    # Looping through all the frames\n",
    "    while cap.isOpened():  # making sure it is reading frames\n",
    "\n",
    "        # Reading the frames\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # # Resizing every frame to a commom value\n",
    "        # frame = cv2.resize(frame, (256, 256))\n",
    "\n",
    "        # Making detections\n",
    "        image, results = landmark_detection(frame, holistic)\n",
    "\n",
    "        # Draw landmarks\n",
    "\n",
    "        # Show to screen\n",
    "        cv2.imshow(\"Video\", image)\n",
    "                \n",
    "        # Extracting landmarks\n",
    "        landmarks_list_np, lh_visible, rh_visible = landmark_extraction(results)\n",
    "        landmarks_list.append(landmarks_list_np)\n",
    "\n",
    "        if lh_visible == 1 or rh_visible == 1:\n",
    "            landmarks_array = np.concatenate(landmarks_list, axis=0)\n",
    "            sign_status = 1\n",
    "            \n",
    "        if lh_visible == 0 and rh_visible == 0 and sign_status == 1:\n",
    "            # Predictions\n",
    "            x_test = np.expand_dims(preprocess_landmarks(landmarks_array), axis=0)\n",
    "\n",
    "            # Making predictions\n",
    "            predicted_label = np.argmax(model.predict(x_test))\n",
    "            predicted_word = np.array([list(sign_dict.keys())[list(sign_dict.values()).index(predicted_label)]])\n",
    "            print(predicted_label, predicted_word)\n",
    "            \n",
    "            landmarks_list = []\n",
    "            sign_status = 0\n",
    "            del landmarks_list_np, landmarks_array\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
