{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Landmarks extraction using OpenCV and Mediapipe Holistic\n",
    "\n",
    "This script will import the videos using OpenCV and extract the landmarks using Mediapipe Holistic.\n",
    "The Mediapipe Holistic model will extract the keypoints from the following models:\n",
    "- Pose Landmark.\n",
    "- Hand Landmark (for both hands).\n",
    "- Face Landmark.\n",
    "\n",
    "Created by: Marcus Vinicius da Silva Fernandes. 2023-06-11.\n",
    "\n",
    "#### References:\n",
    "- https://mediapipe-studio.webapps.google.com/home\n",
    "- https://www.geeksforgeeks.org/face-and-hand-landmarks-detection-using-python-mediapipe-opencv/\n",
    "- https://www.youtube.com/watch?v=pG4sUNDOZFg\n",
    "- https://www.youtube.com/watch?v=0JU3kpYytuQ\n",
    "- https://arrow.apache.org/docs/python/index.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up of the Holistic model by Mediapipe\n",
    "\n",
    "It will run the following models:\n",
    "- pose_landmarks\n",
    "- face_landmarks\n",
    "- left_hand_landmarks\n",
    "- right_hand_landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic  # for landmarks detection."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing the videos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the paths of folders to locate the videos and the list (csv file) that associates the name of the video to the corresponding word in English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up of the videos path\n",
    "videos_path = '/Users/marcus/Library/CloudStorage/OneDrive-Personal/Documentos/Loyalist_College/AISC2006/WLASL_videos_clean_1000v/'\n",
    "\n",
    "# Set up of the extracted landmarks save path\n",
    "landmarks_path = '/Users/marcus/Library/CloudStorage/OneDrive-Personal/Documentos/Loyalist_College/AISC2006/resized_extracted_landmarks_xy/'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Landmarks detection function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to detect the landmarks in each frame or image\n",
    "def landmark_detection(frame, model):\n",
    "    # Color conversion because mediapipe's landmark detection model expects RGB frames as input.\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # color conversion BGR to RGB.\n",
    "    frame.flags.writeable = False  # frame is not writeable.\n",
    "    results = model.process(frame)  # landmarks detection.\n",
    "    frame.flags.writeable = True  # frame is writeable.\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)  # color conversion RGB to BGR.\n",
    "    return frame, results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Landmarks coordinates extraction function\n",
    "\n",
    "It will :\n",
    "- Extract the coordinates from the parameter 'results'.\n",
    "- Only x and y coordinates are saved\n",
    "- Store them into a numpy array.\n",
    "    - 'flatten' function will write all the coordinates in a single array, so the length will be:\n",
    "        - Pose: 2 coordinates x 33 landmarks = 66 values.\n",
    "        - Left hand: 2 coordinates x 21 landmarks = 42 values.\n",
    "        - Right hand: 2 coordinates x 21 landmarks = 42 values.\n",
    "        - Face: 2 coordinates x 468 landmarks = 936 values.\n",
    "        - Each row (each frame) will have a total of 1086 values after concatenation.\n",
    "    - It will store zeros if the parameter 'results' has no value for the model (e.g. it can happen when the hand was not visible and therefore was not identified)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract the coordinates of the detected landmarks\n",
    "def landmark_extraction(results):\n",
    "    pose = np.array([[coordinate.x, coordinate.y] for coordinate in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33 * 2)\n",
    "    left_hand = np.array([[coordinate.x, coordinate.y] for coordinate in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21 * 2)\n",
    "    right_hand = np.array([[coordinate.x, coordinate.y] for coordinate in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21 * 2)\n",
    "    face = np.array([[coordinate.x, coordinate.y] for coordinate in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468 * 2)\n",
    "    return np.concatenate([pose, left_hand, right_hand, face])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert and store the numpy array into parquet file function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert and store the numpy array into parquet file\n",
    "def parquet_writer(np_array, video_id):\n",
    "    np_array_flat = np_array.flatten()\n",
    "    pa_array = pa.array(np_array_flat)  # converting the numpy array into a pyarrow array\n",
    "    table = pa.Table.from_arrays([pa_array], names=[video_id])  # creating a table\n",
    "    writer = pq.ParquetWriter(landmarks_path + video_id + '.parquet', table.schema)  # Create a Parquet file writer\n",
    "    writer.write_table(table)  # Write the table to the Parquet file\n",
    "    writer.close()  # Close the Parquet file writer\n",
    "    return"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main code for detection and extraction\n",
    "- Loading the videos and converting them into frames by OpenCV.\n",
    "- For each frame, the function landmark_detection will be called to make the detections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "[NULL @ 0x299abcf40] Invalid NAL unit size (71678 > 10776).\n",
      "[NULL @ 0x299abcf40] missing picture in access unit with size 10780\n",
      "[h264 @ 0x299a76870] Invalid NAL unit size (71678 > 10776).\n",
      "[h264 @ 0x299a76870] Error splitting the input into NAL units.\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x299ab2da0] stream 1, offset 0x2a27a7: partial file\n",
      "[h264 @ 0x114fd6be0] Invalid NAL unit size (745 > 472).\n",
      "[h264 @ 0x114fd6be0] Error splitting the input into NAL units.\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x15fd65b40] stream 1, offset 0x3b468: partial file\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x15fd65b40] stream 1, offset 0x3b7d3: partial file\n"
     ]
    }
   ],
   "source": [
    "# Capturing the video frames from the files in the video path\n",
    "for item in os.listdir(videos_path):\n",
    "    if item.endswith('.mp4'):  # working with video files only\n",
    "        cap = cv2.VideoCapture(videos_path + item)\n",
    "\n",
    "        # List that will receive the landmark's coordinates for each video\n",
    "        landmarks_list = []\n",
    "\n",
    "        # Set mediapipe model\n",
    "        with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "                \n",
    "            # Looping through all the frames\n",
    "            while cap.isOpened():  # making sure it is reading frames\n",
    "\n",
    "                # Reading the frames\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:  # in case a frame wasn't successfully read or the last frame was already worked on\n",
    "                    break\n",
    "\n",
    "                # Resizing every frame to a commom value\n",
    "                frame = cv2.resize(frame, (256, 256))\n",
    "\n",
    "                # Making detections\n",
    "                image, results = landmark_detection(frame, holistic)\n",
    "                \n",
    "                # Extracting landmarks\n",
    "                # The list for each video will have: 1086 columns (landmark's coordinates) and number of rows equal to the number of frames of the video\n",
    "                landmarks_list.append(landmark_extraction(results))\n",
    "\n",
    "                cv2.waitKey(10)\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "\n",
    "        # Saving the NumPy array\n",
    "        np.save(landmarks_path + '/' + item.split(\".mp4\")[0], np.array(landmarks_list))\n",
    "        \n",
    "        # Converting and storing the array into parquet file\n",
    "        # parquet_writer(np.array(landmarks_list), item.split('.mp4')[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
