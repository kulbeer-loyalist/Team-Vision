{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Landmarks extraction using OpenCV and Mediapipe Holistic\n",
    "\n",
    "This script will import the videos using OpenCV and extract the landmarks using Mediapipe Holistic.\n",
    "The Mediapipe Holistic model will extract the keypoints from the following models:\n",
    "- Pose Landmark.\n",
    "- Hand Landmark (for both hands).\n",
    "- Face Landmark.\n",
    "\n",
    "Created by: Marcus Vinicius da Silva Fernandes. 2023-06-11.\n",
    "\n",
    "#### References:\n",
    "- https://mediapipe-studio.webapps.google.com/home\n",
    "- https://www.geeksforgeeks.org/face-and-hand-landmarks-detection-using-python-mediapipe-opencv/\n",
    "- https://www.youtube.com/watch?v=pG4sUNDOZFg\n",
    "- https://www.youtube.com/watch?v=0JU3kpYytuQ\n",
    "- https://arrow.apache.org/docs/python/index.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up of the Holistic model by Mediapipe\n",
    "\n",
    "It will run the following models:\n",
    "- pose_landmarks\n",
    "- face_landmarks\n",
    "- left_hand_landmarks\n",
    "- right_hand_landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic  # for landmarks detection."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing the videos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the paths of folders to locate the videos and the list (csv file) that associates the name of the video to the corresponding word in English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up of the videos path\n",
    "videos_path = '/Users/marcus/Library/CloudStorage/OneDrive-Personal/Documentos/Loyalist_College/AISC2006/predictions_wlasl_for_gd/videos/'\n",
    "\n",
    "# Set up of the extracted landmarks save path\n",
    "landmarks_path = '/Users/marcus/Library/CloudStorage/OneDrive-Personal/Documentos/Loyalist_College/AISC2006/predictions_wlasl_for_gd/landmarks/'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Landmarks detection function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to detect the landmarks in each frame or image\n",
    "def landmark_detection(frame, model):\n",
    "    # Color conversion because mediapipe's landmark detection model expects RGB frames as input.\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # color conversion BGR to RGB.\n",
    "    frame.flags.writeable = False  # frame is not writeable.\n",
    "    results = model.process(frame)  # landmarks detection.\n",
    "    frame.flags.writeable = True  # frame is writeable.\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)  # color conversion RGB to BGR.\n",
    "    return frame, results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Landmarks coordinates extraction function\n",
    "\n",
    "It will :\n",
    "- Extract the coordinates from the parameter 'results'.\n",
    "- Only x and y coordinates are saved\n",
    "- Store them into a numpy array.\n",
    "    - 'flatten' function will write all the coordinates in a single array, so the length will be:\n",
    "        - Pose: 2 coordinates x 33 landmarks = 66 values.\n",
    "        - Left hand: 2 coordinates x 21 landmarks = 42 values.\n",
    "        - Right hand: 2 coordinates x 21 landmarks = 42 values.\n",
    "        - Face: 2 coordinates x 468 landmarks = 936 values.\n",
    "        - Each row (each frame) will have a total of 1086 values after concatenation.\n",
    "    - It will store zeros if the parameter 'results' has no value for the model (e.g. it can happen when the hand was not visible and therefore was not identified)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract the coordinates of the detected landmarks\n",
    "def landmark_extraction(results, frame_idx):\n",
    "    \n",
    "    if results.face_landmarks:\n",
    "        face = np.array([[frame_idx, str(frame_idx) + '-face-' + str(idx), 'face', idx, coordinate.x, coordinate.y, coordinate.z] for idx, coordinate in enumerate(results.face_landmarks.landmark)])\n",
    "    else:\n",
    "        face = np.array([[frame_idx, str(frame_idx) + '-face-' + str(idx), 'face', idx, 0, 0, 0] for idx in range(468)])\n",
    "\n",
    "    if results.left_hand_landmarks:\n",
    "        left_hand = np.array([[frame_idx, str(frame_idx) + '-left_hand-' + str(idx), 'left_hand', idx, coordinate.x, coordinate.y, coordinate.z] for idx, coordinate in enumerate(results.left_hand_landmarks.landmark)])\n",
    "    else:\n",
    "        left_hand = np.array([[frame_idx, str(frame_idx) + '-left_hand-' + str(idx), 'left_hand', idx, 0, 0, 0] for idx in range(21)])\n",
    "\n",
    "    if results.pose_landmarks:\n",
    "        pose = np.array([[frame_idx, str(frame_idx) + '-pose-' + str(idx), 'pose', idx, coordinate.x, coordinate.y, coordinate.z] for idx, coordinate in enumerate(results.pose_landmarks.landmark)])\n",
    "    else:\n",
    "        pose = np.array([[frame_idx, str(frame_idx) + '-pose-' + str(idx), 'pose', idx, 0, 0, 0] for idx in range(33)])\n",
    "    \n",
    "    if results.right_hand_landmarks:\n",
    "        right_hand = np.array([[frame_idx, str(frame_idx) + '-right_hand-' + str(idx), 'right_hand', idx, coordinate.x, coordinate.y, coordinate.z] for idx, coordinate in enumerate(results.right_hand_landmarks.landmark)])\n",
    "    else:\n",
    "        right_hand = np.array([[frame_idx, str(frame_idx) + '-right_hand-' + str(idx), 'right_hand', idx, 0, 0, 0] for idx in range(21)])\n",
    "    \n",
    "    return np.concatenate([face, left_hand, pose, right_hand])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main code for detection and extraction\n",
    "- Loading the videos and converting them into frames by OpenCV.\n",
    "- For each frame, the function landmark_detection will be called to make the detections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "# Capturing the video frames from the files in the video path\n",
    "for item in os.listdir(videos_path):\n",
    "    if item.endswith('.mp4'):  # working with video files only\n",
    "        cap = cv2.VideoCapture(videos_path + item)\n",
    "\n",
    "        # List that will receive the landmark's coordinates for each video\n",
    "        landmarks_list = []\n",
    "        frame_idx = 1\n",
    "        \n",
    "        # Set mediapipe model\n",
    "        with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "                \n",
    "            # Looping through all the frames\n",
    "            while cap.isOpened():  # making sure it is reading frames\n",
    "\n",
    "                # Reading the frames\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:  # in case a frame wasn't successfully read or the last frame was already worked on\n",
    "                    break\n",
    "\n",
    "                # Resizing every frame to a commom value\n",
    "                frame = cv2.resize(frame, (256, 256))\n",
    "\n",
    "                # Making detections\n",
    "                image, results = landmark_detection(frame, holistic)\n",
    "                \n",
    "                # Extracting landmarks\n",
    "                # The list for each video will have: 1086 columns (landmark's coordinates) and number of rows equal to the number of frames of the video\n",
    "                landmarks_list.append(landmark_extraction(results, frame_idx))\n",
    "                \n",
    "                landmarks_array = np.concatenate(landmarks_list, axis=0)\n",
    "                frame_idx += 1\n",
    "\n",
    "                cv2.waitKey(10)\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "\n",
    "        index_col = np.arange(543 * (frame_idx - 1)).reshape(-1, 1)\n",
    "        landmarks_array = np.hstack((index_col, landmarks_array))\n",
    "\n",
    "        # Saving the NumPy array\n",
    "        # np.save(landmarks_path + '/' + item.split(\".mp4\")[0], landmarks_array)\n",
    "        \n",
    "        # Storing the array into parquet file\n",
    "        column_names = ['index', 'frame', 'row_id', 'type', 'landmark_index', 'x', 'y', 'z']\n",
    "        df = pd.DataFrame(landmarks_array, columns=column_names)  # creating a dataframe\n",
    "        df.to_parquet(landmarks_path + '/' + item.split(\".mp4\")[0] + '.parquet', index=False)  # saving the dataframe in a parquet file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
