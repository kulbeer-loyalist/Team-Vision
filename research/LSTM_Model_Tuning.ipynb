{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action Recognition - LSTM Model Implementation Study\n",
    "\n",
    "This script implements and tune a LSTM model for ASL.\n",
    "\n",
    "Created by:\n",
    "- Marcus Vinicius da Silva Fernandes.\n",
    "\n",
    "2023-06-12.\n",
    "\n",
    "#### References:\n",
    "- https://www.youtube.com/watch?v=pG4sUNDOZFg\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Masking, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "# import keras_tuner as kt\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing the landmarks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the paths of folders to locate the landmarks and the list (csv file) that associates the name of the video to the corresponding word in English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up of the extracted landmarks save path\n",
    "# landmarks_path = '/Users/marcus/Library/CloudStorage/OneDrive-Personal/Documentos/Loyalist_College/AISC2006/cleaned_extracted_landmarks_xy/'\n",
    "landmarks_path = 'C:/Users/marcu/OneDrive/Documentos/Loyalist_College/AISC2006/'\n",
    "train_folder = 'C:/Users/marcu/OneDrive/Documentos/Loyalist_College/AISC2006/train_dir/'\n",
    "valid_folder = 'C:/Users/marcu/OneDrive/Documentos/Loyalist_College/AISC2006/valid_dir/'\n",
    "test_folder =  'C:/Users/marcu/OneDrive/Documentos/Loyalist_College/AISC2006/test_dir/'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of the dictionary to associate the videos and the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening the file dataset_analysis.csv to load the association of landmark ids to words and its number of frames\n",
    "id_dict = {}  # initializing the dictionary that will receive the data\n",
    "num_frames = []  # initializing the list that will contain the number of frames of each landmark\n",
    "\n",
    "with open(landmarks_path + \"Updated Dataset2.csv\", \"r\") as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)  # reading the data\n",
    "    next(csv_reader)  # to skip the header\n",
    "    for row in csv_reader:\n",
    "        if int(row[0]) <= 10000:\n",
    "            id_dict['0' * (5 - len(row[0])) + row[0]] = row[1]  # storing the content into a dictionary\n",
    "        else:\n",
    "            id_dict[row[0]] = row[1]  # storing the content into a dictionary\n",
    "        num_frames.append(int(row[7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number of frames of all the landmarks = 149\n",
      "Minimum number of frames of all the landmarks = 26\n"
     ]
    }
   ],
   "source": [
    "# Maximum number of frames of all the landmarks\n",
    "max_num_frames = max(num_frames)\n",
    "print('Maximum number of frames of all the landmarks =', max_num_frames)\n",
    "\n",
    "# Minimum number of frames of all the landmarks\n",
    "min_num_frames = min(num_frames)\n",
    "print('Minimum number of frames of all the landmarks =', min_num_frames)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shaping the data for the LSTM model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desired number of frames\n",
    "- Each video will be reshaped to have the number of rows (or frames) equal to the desired number of frames defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FRAMES = 60"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of the X array\n",
    "\n",
    "- Time-based sampling: we will reduce the dimension of the array to the desired NUM_FRAMES.\n",
    "- Padding the videos: we will add rows with zeros to increase the dimension of the array to the desired NUM_FRAMES.\n",
    "- No change: the array already has the desired NUM_FRAMES."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def load_landmarks(path):\n",
    "    videos, labels = [], []\n",
    "    i = 0\n",
    "\n",
    "    for item in os.listdir(path):\n",
    "        if item.endswith('.npy'):  # working with npy files only\n",
    "            if i == 80:\n",
    "                return np.array(videos), labels\n",
    "            data = np.load(os.path.join(path, item))  # loading the numpy array into memory\n",
    "            if data.shape[0] > NUM_FRAMES:  # time-based sampling\n",
    "                indices = np.arange(0, data.shape[0], data.shape[0] // NUM_FRAMES)[:NUM_FRAMES]\n",
    "                data = data[indices]\n",
    "                videos.append(data)\n",
    "            elif data.shape[0] < NUM_FRAMES:  # padding the videos\n",
    "                data = np.pad(data, ((0, NUM_FRAMES - data.shape[0]), (0, 0)), mode='constant')\n",
    "                videos.append(data)\n",
    "            else:  # no change\n",
    "                videos.append(data)\n",
    "            labels.append(id_dict[item.split('.npy')[0]])\n",
    "            # i += 1\n",
    "\n",
    "    return np.array(videos), np.array(labels)\n",
    "\n",
    "x_train, y_train = load_landmarks(train_folder)\n",
    "x_val, y_val = load_landmarks(valid_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_landmarks(path):\n",
    "    videos, labels = [], []\n",
    "    i = 0\n",
    "\n",
    "    for item in os.listdir(path):\n",
    "        if item.endswith('.npy') and item.split('.npy')[0] in id_dict:  # working with npy files only\n",
    "            if i == 800:\n",
    "                return np.array(videos), labels\n",
    "            data = np.load(os.path.join(path, item))  # loading the numpy array into memory\n",
    "            if data.shape[0] > NUM_FRAMES:  # time-based sampling\n",
    "                indices = np.arange(0, data.shape[0], data.shape[0] // NUM_FRAMES)[:NUM_FRAMES]\n",
    "                data = data[indices]\n",
    "                videos.append(data)\n",
    "            elif data.shape[0] < NUM_FRAMES:  # padding the videos\n",
    "                data = np.pad(data, ((0, NUM_FRAMES - data.shape[0]), (0, 0)), mode='constant')\n",
    "                videos.append(data)\n",
    "            else:  # no change\n",
    "                videos.append(data)\n",
    "            labels.append(id_dict[item.split('.npy')[0]])\n",
    "            # i += 1\n",
    "\n",
    "    return np.array(videos), np.array(labels)\n",
    "\n",
    "x_train, y_train = load_landmarks(train_folder)\n",
    "x_val, y_val = load_landmarks(valid_folder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of the Y array\n",
    "\n",
    "- One-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all_labels = np.concatenate((y_train, y_val), axis=0)\n",
    "\n",
    "labels_unique = np.unique(all_labels)\n",
    "\n",
    "labels_encoded = []\n",
    "for i in all_labels:\n",
    "    labels_encoded = np.append(labels_encoded, np.where(labels_unique == i))\n",
    "\n",
    "labels_encoded = to_categorical(labels_encoded).astype(int)\n",
    "\n",
    "y_train = labels_encoded[ : len(y_train)]\n",
    "y_val = labels_encoded[len(y_train) : ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = np.concatenate((y_train, y_val), axis=0)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(all_labels)\n",
    "\n",
    "y_train_encoded = label_encoder.transform(y_train)\n",
    "y_val_encoded = label_encoder.transform(y_val)\n",
    "\n",
    "num_classes = len(label_encoder.classes_)\n",
    "y_train = to_categorical(y_train_encoded, num_classes=num_classes)\n",
    "y_val = to_categorical(y_val_encoded, num_classes=num_classes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Architecture definition and tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Masking(mask_value=0, input_shape=(x_train.shape[1], x_train.shape[2])))\n",
    "    \n",
    "    # Add LSTM layers\n",
    "    for i in range(hp.Int('num_lstm_layers', min_value=1, max_value=3)):\n",
    "        return_sequences = (i < hp.Int('num_lstm_layers', min_value=1, max_value=3) - 1)\n",
    "        model.add(LSTM(units=hp.Int(f'units_lstm_{i}', min_value=32, max_value=256, step=32),\n",
    "                       activation=hp.Choice(f'activation_lstm_{i}', values=['relu', 'tanh']),\n",
    "                       recurrent_dropout=hp.Float(f'recurrent_dropout_{i}', min_value=0.1, max_value=0.5, step=0.1),\n",
    "                       return_sequences=return_sequences))\n",
    "        \n",
    "        # Add dropout between LSTM layers\n",
    "        dropout_rate = hp.Float(f'dropout_rate_lstm_{i}', min_value=0.1, max_value=0.5, step=0.1)\n",
    "        model.add(Dropout(rate=dropout_rate))\n",
    "    \n",
    "    # Add Dense layers\n",
    "    for j in range(hp.Int('num_dense_layers', min_value=1, max_value=3)):\n",
    "        model.add(Dense(units=hp.Int(f'units_dense_{j}', min_value=32, max_value=256, step=32),\n",
    "                        activation=hp.Choice(f'activation_dense_{j}', values=['relu', 'sigmoid'])))\n",
    "        \n",
    "        # Add dropout between Dense layers\n",
    "        dropout_rate = hp.Float(f'dropout_rate_dense_{j}', min_value=0.1, max_value=0.5, step=0.1)\n",
    "        model.add(Dropout(rate=dropout_rate))\n",
    "    \n",
    "    model.add(Dense(units=y_train.shape[1], activation='softmax'))\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4]))\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "tuner = kt.Hyperband(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_epochs=50,\n",
    "    factor=3,\n",
    "    directory='keras_tuner_dir',\n",
    "    project_name='lstm_hyperparameter_tuning'\n",
    ")\n",
    "\n",
    "tuner.search(x_train, y_train, epochs=50, validation_data=(x_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_hps = tuner.get_best_hyperparameters(num_trials=3)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_model = tuner.hypermodel.build(best_hps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_model.fit(x_train, to_categorical(y_train), epochs=10, validation_data=(x_val, to_categorical(y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "# Second try - masking layer added\n",
    "model = Sequential()\n",
    "model.add(Masking(mask_value=0, input_shape=(x_train.shape[1], x_train.shape[2])))  # Input shape with variable-length sequences\n",
    "model.add(LSTM(128, activation='relu'))\n",
    "model.add(Dropout(rate=0.1))\n",
    "model.add(Dense(y_train.shape[1], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-14 00:30:18.849556: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 13s 7s/step - loss: 3.4476 - accuracy: 0.0645 - val_loss: 3.3784 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 11s 6s/step - loss: 3.2636 - accuracy: 0.0968 - val_loss: 3.0815 - val_accuracy: 0.0556\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 11s 6s/step - loss: 3.0475 - accuracy: 0.0806 - val_loss: 3.0516 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 11s 6s/step - loss: 3.0932 - accuracy: 0.0484 - val_loss: 2.8903 - val_accuracy: 0.1111\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 11s 6s/step - loss: 2.9048 - accuracy: 0.0968 - val_loss: 2.9571 - val_accuracy: 0.0556\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 11s 6s/step - loss: 2.8422 - accuracy: 0.0645 - val_loss: 3.0312 - val_accuracy: 0.0556\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 11s 6s/step - loss: 2.7902 - accuracy: 0.0645 - val_loss: 3.1134 - val_accuracy: 0.0556\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 11s 6s/step - loss: 2.8125 - accuracy: 0.1129 - val_loss: 3.0955 - val_accuracy: 0.0556\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 11s 6s/step - loss: 2.7685 - accuracy: 0.0968 - val_loss: 3.0469 - val_accuracy: 0.0556\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 11s 6s/step - loss: 2.8156 - accuracy: 0.0806 - val_loss: 2.9995 - val_accuracy: 0.1111\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 11s 6s/step - loss: 2.8148 - accuracy: 0.1129 - val_loss: 2.9859 - val_accuracy: 0.0556\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 11s 6s/step - loss: 2.8080 - accuracy: 0.0645 - val_loss: 3.0183 - val_accuracy: 0.0556\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 11s 6s/step - loss: 2.7999 - accuracy: 0.0484 - val_loss: 3.0906 - val_accuracy: 0.0556\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 11s 6s/step - loss: 2.8095 - accuracy: 0.1129 - val_loss: 3.1946 - val_accuracy: 0.0556\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 11s 6s/step - loss: 2.7983 - accuracy: 0.0806 - val_loss: 3.0999 - val_accuracy: 0.0556\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 11s 5s/step - loss: 2.7641 - accuracy: 0.0806 - val_loss: 3.0653 - val_accuracy: 0.1667\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 11s 6s/step - loss: 2.7500 - accuracy: 0.0806 - val_loss: 3.0878 - val_accuracy: 0.0556\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 11s 6s/step - loss: 2.7451 - accuracy: 0.0968 - val_loss: 3.1330 - val_accuracy: 0.0556\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 11s 6s/step - loss: 2.7627 - accuracy: 0.0645 - val_loss: 3.0958 - val_accuracy: 0.0556\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 11s 6s/step - loss: 2.7603 - accuracy: 0.0968 - val_loss: 3.0348 - val_accuracy: 0.0556\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 11s 6s/step - loss: 2.7561 - accuracy: 0.1290 - val_loss: 3.0127 - val_accuracy: 0.0556\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 11s 6s/step - loss: 2.7415 - accuracy: 0.0968 - val_loss: 3.0848 - val_accuracy: 0.0556\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 11s 5s/step - loss: 2.7461 - accuracy: 0.0484 - val_loss: 3.1136 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 12s 6s/step - loss: 2.7197 - accuracy: 0.1129 - val_loss: 3.1234 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 12s 6s/step - loss: 2.7252 - accuracy: 0.1129 - val_loss: 3.1498 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 11s 6s/step - loss: 2.7249 - accuracy: 0.1290 - val_loss: 3.0729 - val_accuracy: 0.0556\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 11s 6s/step - loss: 2.7204 - accuracy: 0.0806 - val_loss: 3.0015 - val_accuracy: 0.0556\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 12s 6s/step - loss: 2.6859 - accuracy: 0.1129 - val_loss: 2.9744 - val_accuracy: 0.0556\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 11s 6s/step - loss: 2.6493 - accuracy: 0.1129 - val_loss: 3.1100 - val_accuracy: 0.0556\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 12s 6s/step - loss: 2.6587 - accuracy: 0.1129 - val_loss: 3.2061 - val_accuracy: 0.0556\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 11s 6s/step - loss: 2.6831 - accuracy: 0.1613 - val_loss: 3.0953 - val_accuracy: 0.0556\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 11s 6s/step - loss: 2.6342 - accuracy: 0.1613 - val_loss: 3.0018 - val_accuracy: 0.0556\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 12s 6s/step - loss: 2.5851 - accuracy: 0.1290 - val_loss: 3.0112 - val_accuracy: 0.0556\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 12s 7s/step - loss: 2.5843 - accuracy: 0.1774 - val_loss: 3.1969 - val_accuracy: 0.0556\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 12s 6s/step - loss: 2.6170 - accuracy: 0.1613 - val_loss: 3.1526 - val_accuracy: 0.0556\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 12s 6s/step - loss: 2.5760 - accuracy: 0.1774 - val_loss: 3.1860 - val_accuracy: 0.0556\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 11s 6s/step - loss: 2.6246 - accuracy: 0.1129 - val_loss: 3.0006 - val_accuracy: 0.0556\n",
      "Epoch 38/300\n",
      "1/2 [==============>...............] - ETA: 5s - loss: 2.5745 - accuracy: 0.1562"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs=300, validation_data=(x_val, y_val), callbacks=[tb_callback])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
